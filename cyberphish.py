# -*- coding: utf-8 -*-
"""cyberphish.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mu_BGKYSn5nrR93ksd-LAPSUjvd94JNS
"""

#importing necessary libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import f1_score as f1
from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,f1_score

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from google.colab import drive
drive.mount('/content/drive')

data_dir = '/content/drive/My drive/SAC10495/'

#loading the dataset

df = pd.read_csv('/content/drive/My Drive/SAC10495/dataset.csv')
pd.set_option('display.max_columns', None)
df

df.info()

df['Result'].value_counts()

"""1 means legitimate, 0 is suspicious and -1 is phishing."""

df['Page_Rank'].value_counts()

"""PageRank PageRank is a value ranging from “0” to “1”. PageRank aims to measure how important a webpage is on the Internet. The greater the PageRank value the more important the webpage. In our datasets, we find that about 95% of phishing webpages have no PageRank. Moreover, we find that the remaining 5% of phishing webpages may reach a PageRank value up to “0.2”. Rule: IF {PageRank<0.2 → Phishing {Otherwise → Legitimate"""

df['Google_Index'].value_counts()

"""Google Index This feature examines whether a website is in Google’s index or not. When a site is indexed by Google, it is displayed on search results (Webmaster resources, 2014). Usually, phishing webpages are merely accessible for a short period and as a result, many phishing webpages may not be found on the Google index. Rule: IF {Webpage Indexed by Google → Legitimate {Otherwise → Phishing"""

df['URLURL_Length'].value_counts()

"""Long URL to Hide the Suspicious Part Phishers can use long URL to hide the doubtful part in the address bar. For example: http://federmacedoadv.com.br/3f/aze/ab51e2e319e51502f416dbe46b773a5e/?cmd=_home&amp;dispatch=11004d58f5b74f8dc1e7c2e8dd4105e811004d58f5b74f8dc1e7c2e8dd4105e8@phishing.website.html To ensure accuracy of our study, we calculated the length of URLs in the dataset and produced an average URL length. The results showed that if the length of the URL is greater than or equal 54 characters then the URL classified as phishing. By reviewing our dataset we were able to find 1220 URLs lengths equals to 54 or more which constitute 48.8% of the total dataset size. We have been able to update this feature rule by using a method based on frequency and thus improving upon its accuracy. RULE: IF {URL length<54 → feature = Legitimate {else if URL length≥54 and ≤75 → feature = Suspicious {otherwise→ feature = Phishing"""

df['having_At_Symbol'].value_counts()

"""URL’s having “@” Symbol Using “@” symbol in the URL leads the browser to ignore everything preceding the “@” symbol and the real address often follows the “@” symbol. RULE: IF {Url Having @ Symbol→ Phishing {Otherwise→ Legitimate


"""

df['double_slash_redirecting'].value_counts()

"""Redirecting using “//” The existence of “//” within the URL path means that the user will be redirected to another website. An example of such URL’s is: “http://www.legitimate.com//http://www.phishing.com”. We examin the location where the “//” appears. We find that if the URL starts with “HTTP”, that means the “//” should appear in the sixth position. However, if the URL employs “HTTPS” then the “//” should appear in seventh position. RULE: IF {The Position of the Last Occurrence of "//\" " in the URL > 7→ Phishing {Otherwise→ Legitimate}"""

df['Prefix_Suffix'].value_counts()

"""Adding Prefix or Suffix Separated by (-) to the Domain The dash symbol is rarely used in legitimate URLs. Phishers tend to add prefixes or suffixes separated by (-) to the domain name so that users feel that they are dealing with a legitimate webpage. For example http://www.Confirme-paypal.com/. RULE: IF {Domain Name Part Includes (-) Symbol → Phishing {Otherwise → Legitimate"""

df.columns

selected_features=['URLURL_Length', 'having_At_Symbol','double_slash_redirecting', 'Prefix_Suffix', 'Page_Rank', 'Google_Index','Result']
df2 = df[selected_features]
df2.head()

new_columns = {'URLURL_Length': 'URL_Length', 'Prefix_Suffix':'HavingHyphen'}
df2= df2.rename(columns=new_columns)

df2 = df2.drop_duplicates()
df2.shape

"""There are total 107 different combinations, this may sounds really less but keep in mind that all the features and labels has combination of only two or three unique values.

The less number will generalize more but detection time will be alot faster.
"""

df2.isna().sum()

df2['Result'].value_counts(normalize=True)

"""1 means legitimate, and -1 is phishing."""

X = df2.drop('Result', axis=1)
y = df2['Result']

#@title Feature Extraction of Variational Auto Encoder Model

"""Autoencoder is a type of neural network that can be used to learn a compressed representation of raw data.

An autoencoder is composed of an encoder and a decoder sub-models. The encoder compresses the input and the decoder attempts to recreate the input from the compressed version provided by the encoder. After training, the encoder model is saved and the decoder is discarded.

The encoder can then be used as a data preparation technique to perform feature extraction on raw data that can be used to train a different machine learning model.
"""

print(X.shape, y.shape)

from sklearn.preprocessing import MinMaxScaler
# split into train test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
# scale data
t = MinMaxScaler()
t.fit(X_train)
X_train = t.transform(X_train)
X_test = t.transform(X_test)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.utils import plot_model
n_inputs=X_train.shape[1]
# define encoder
visible = Input(shape=(n_inputs,))
# encoder level 1
e = Dense(n_inputs*2)(visible)
e = BatchNormalization()(e)
e = LeakyReLU()(e)
# encoder level 2
e = Dense(n_inputs)(e)
e = BatchNormalization()(e)
e = LeakyReLU()(e)
# bottleneck
n_bottleneck = n_inputs
bottleneck = Dense(n_bottleneck)(e)

# define decoder, level 1
d = Dense(n_inputs)(bottleneck)
d = BatchNormalization()(d)
d = LeakyReLU()(d)
# decoder level 2
d = Dense(n_inputs*2)(d)
d = BatchNormalization()(d)
d = LeakyReLU()(d)

# output layer
output = Dense(n_inputs, activation='linear')(d)

# define autoencoder model
model = Model(inputs=visible, outputs=output)

# compile autoencoder model
model.compile(optimizer='adam', loss='mse')

# plot the autoencoder
plot_model(model, 'autoencoder_no_compress.png', show_shapes=True)

"""The image above shows a plot of the autoencoder.

We will define the encoder to have two hidden layers, the first with two times the number of inputs 6 and the second with the same number of inputs (12), followed by the bottleneck layer with the same number of inputs as the dataset (6).

The decoder will be defined with a similar structure, although in reverse.

It will have two hidden layers, the first with the number of inputs in the dataset (6) and the second with double the number of inputs (12). The output layer will have the same number of nodes as there are columns in the input data and will use a linear activation function to output numeric values.
"""

# fit the autoencoder model to reconstruct input
history = model.fit(X_train, X_train, epochs=200, batch_size=32, verbose=2, validation_data=(X_test,X_test))

# plot loss
plt.plot(history.history['loss'], label='train',color='blue')
plt.plot(history.history['val_loss'], label='test',color='red')
plt.xlabel('Epochs',fontweight='bold')
plt.ylabel('Loss',fontweight='bold')
plt.title('Training and Testing Loss for Variational Auto Encoder',fontweight='bold')
plt.legend()
plt.show()

# define an encoder model (without the decoder)
encoder = Model(inputs=visible, outputs=bottleneck)
plot_model(encoder, 'encoder_no_compress.png', show_shapes=True)

# save the encoder model to drive
encoder.save('/content/drive/My Drive/SAC10495/encoder_model.h5')

"""So far, so good. We know how to develop an autoencoder without compression.

Next, let’s change the configuration of the model so that the bottleneck layer has half the number of nodes
"""

# bottleneck
n_bottleneck = round(float(n_inputs) / 2.0)
bottleneck = Dense(n_bottleneck)(e)

# define decoder, level 1
d = Dense(n_inputs)(bottleneck)
d = BatchNormalization()(d)
d = LeakyReLU()(d)
# decoder level 2
d = Dense(n_inputs*2)(d)
d = BatchNormalization()(d)
d = LeakyReLU()(d)
# output layer
output = Dense(n_inputs, activation='linear')(d)
# define autoencoder model
model = Model(inputs=visible, outputs=output)
# compile autoencoder model
model.compile(optimizer='adam', loss='mse')

# plot the autoencoder
plot_model(model, 'autoencoder_compress.png', show_shapes=True)

# fit the autoencoder model to reconstruct input
history = model.fit(X_train, X_train, epochs=200, batch_size=32, verbose=2, validation_data=(X_test,X_test))

# plot loss
plt.plot(history.history['loss'], label='train',color='blue')
plt.plot(history.history['val_loss'], label='test',color='red')
plt.xlabel('Epochs',fontweight='bold')
plt.ylabel('Loss',fontweight='bold')
plt.title('Training and Testing Loss for Variational Auto Encoder after Reconstruction',fontweight='bold')
plt.legend()
plt.show()

# define an encoder model (without the decoder)
encoder_reconstructed = Model(inputs=visible, outputs=bottleneck)
plot_model(encoder_reconstructed, 'encoder_compress.png', show_shapes=True)
# save the encoder model to drive
encoder_reconstructed.save('/content/drive/My Drive/SAC10495/encoder_reconstructed_model.h5')

#@title Feature Extraction by Darknet19 Model

X_train.shape[1]

import keras
from keras.layers import Conv2D, Input, concatenate
from keras.layers import LeakyReLU, MaxPooling2D, BatchNormalization,GlobalAveragePooling2D
from keras.models import Model
from keras.activations import softmax
from functools import partial

from keras.layers import Input, Conv2D, LeakyReLU, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Lambda
from keras.models import Model
from keras.activations import softmax
from functools import partial

new_conv = partial(Conv2D, padding="same")

def _base_block(out, x):
    "(3,3), Leaky, Batch"
    x = new_conv(out, (3,3))(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = BatchNormalization()(x)
    return x

def _block_1(out, x):
    """
    output follows:
    out//2, out
    """
    x = new_conv(out//2, (1,1))(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = BatchNormalization()(x)
    x = _base_block(out, x)
    return x

def _block_2(out, x):
    """
    output follows:
    out, out//2, out
    """
    x = _base_block(out, x)
    x = _block_1(out, x)
    return x

def Darknet19():
    input_layer = Input((img_size, img_size, 3))  # Adjusted input shape
    x = _base_block(32, input_layer)
    x = _base_block(64, x)
    x = _block_2(128, x)
    x = _block_2(256, x)
    x = _block_2(512, x)
    x = _block_1(512, x)
    x = _block_2(1024, x)
    x = _block_1(512, x)
    x = new_conv(1, (1,1), activation="linear")(x)
    model = Model(inputs=input_layer, outputs=x)
    return model

def apply_soft(x):
    output = softmax(x)
    return output

def Darknet_classifier():
    base_model = Darknet19()
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    output = Lambda(apply_soft)(x)
    model = Model(inputs=base_model.inputs, outputs=output)
    return model

img_size = X_train.shape[1]
darknet_model = Darknet_classifier()
print(darknet_model.summary())

print('Input layer of X_train.shape[1]:',X_train.shape[1])

#plot the darknet19 model
# Generate the plot
plot_model(Darknet_classifier(), to_file='darknet_classifier_plot.png', show_shapes=True, show_layer_names=True)

print(X_train.shape)

from keras.layers import Input, Dense, LeakyReLU, BatchNormalization
from keras.models import Model
import numpy as np

def create_darknet_model(input_shape):
    input_layer = Input(shape=input_shape)
    x = Dense(32)(input_layer)
    x = LeakyReLU(alpha=0.1)(x)
    x = BatchNormalization()(x)
    x = Dense(64)(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = BatchNormalization()(x)
    x = Dense(128)(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = BatchNormalization()(x)
    x = Dense(256)(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = BatchNormalization()(x)
    x = Dense(512)(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = BatchNormalization()(x)
    x = Dense(512)(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = BatchNormalization()(x)
    x = Dense(1024)(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = BatchNormalization()(x)
    x = Dense(512)(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = BatchNormalization()(x)
    output = Dense(6, activation='linear')(x)  # Adjust output shape if needed
    model = Model(inputs=input_layer, outputs=output)
    return model



input_shape = X_train.shape[1]
darknet_model = create_darknet_model(input_shape)

# Compile the model
darknet_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])

# Train the model
darknet_history = darknet_model.fit(X_train, X_train, epochs=1000, batch_size=32, verbose=2, validation_data=(X_test, X_test))

# Get the training history
loss = darknet_history.history['loss']
val_loss = darknet_history.history['val_loss']

# Plot the training and validation loss
plt.plot(loss, label='Training Loss',color='blue')
plt.plot(val_loss, label='Validation Loss',color='red')
plt.xlabel('Epochs',fontweight='bold')
plt.ylabel('Loss',fontweight='bold')
plt.title('Training and Validation Loss for Darknet19 Model',fontweight='bold')
plt.legend()
plt.show()

import matplotlib.pyplot as plt



# Plot the training loss
plt.figure(figsize=(6, 5))
plt.plot(history.history['loss'], label='VAE Training Loss', color='blue')
plt.plot(darknet_history.history['loss'], label='Darknet Training Loss', color='red')
plt.xlabel('Epochs',fontweight='bold')
plt.ylabel('Loss',fontweight='bold')
plt.title('Training Loss Comparison of two Feature Extraction Models',fontweight='bold')
plt.legend()
plt.show()

# Plot the validation loss
plt.figure(figsize=(6, 5))
plt.plot(history.history['val_loss'], label='VAE Validation Loss', color='blue')
plt.plot(darknet_history.history['val_loss'], label='Darknet Validation Loss', color='red')
plt.xlabel('Epochs',fontweight='bold')
plt.ylabel('Loss',fontweight='bold')
plt.title('Validation Loss Comparison of two Feature Extraction Models',fontweight='bold')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Define grid line style
grid_style = {
    'color': 'gray',
    'linestyle': '--',
    'linewidth': 0.5,
    'alpha': 0.7
}

# Plot the training loss as scatter plot with reduced point size
plt.figure(figsize=(6, 5))
plt.scatter(range(len(history.history['loss'])), history.history['loss'], label='VAE Training Loss', color='blue', s=5)
plt.scatter(range(len(darknet_history.history['loss'])), darknet_history.history['loss'], label='Darknet Training Loss', color='red', s=5)
plt.xlabel('Epochs', fontweight='bold')
plt.ylabel('Loss', fontweight='bold')
plt.title('Training Loss Comparison of two Feature Extraction Models', fontweight='bold')
plt.legend()
plt.grid(True, **grid_style)  # Add styled grid lines
plt.show()

# Plot the validation loss as scatter plot with reduced point size
plt.figure(figsize=(6, 5))
plt.scatter(range(len(history.history['val_loss'])), history.history['val_loss'], label='VAE Validation Loss', color='blue', s=5)
plt.scatter(range(len(darknet_history.history['val_loss'])), darknet_history.history['val_loss'], label='Darknet Validation Loss', color='red', s=5)
plt.xlabel('Epochs', fontweight='bold')
plt.ylabel('Loss', fontweight='bold')
plt.title('Validation Loss Comparison of two Feature Extraction Models', fontweight='bold')
plt.legend()
plt.grid(True, **grid_style)  # Add styled grid lines
plt.show()

# Define the data for the pie charts
vae_train_loss = sum(history.history['loss'])
vae_val_loss = sum(history.history['val_loss'])

darknet_train_loss = sum(darknet_history.history['loss'])
darknet_val_loss = sum(darknet_history.history['val_loss'])

vae_data = [vae_train_loss, vae_val_loss]
darknet_data = [darknet_train_loss, darknet_val_loss]

# Labels for the sections
labels = ['Training Loss', 'Validation Loss']

# Colors for the sections
colors = ['blue', 'orange']

# Plot pie chart for VAE
plt.figure(figsize=(5, 5))
plt.pie(vae_data, labels=labels, autopct='%1.1f%%', colors=colors, startangle=140)
plt.title('VAE Loss Distribution', fontweight='bold')
plt.show()

# Plot pie chart for Darknet
plt.figure(figsize=(5, 5))
plt.pie(darknet_data, labels=labels, autopct='%1.1f%%', colors=colors, startangle=140)
plt.title('Darknet Loss Distribution', fontweight='bold')
plt.show()

# save the encoder model to drive
darknet_model.save('/content/drive/My Drive/SAC10495/darknet_model.h5')

#@title Transformer Based Deep Belief Network
import tensorflow as tf
from tensorflow.keras.layers import Dense, LayerNormalization, MultiHeadAttention, Embedding, GlobalAveragePooling1D

class MultiHeadAttentionLayer(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads):
        super(MultiHeadAttentionLayer, self).__init__()
        self.num_heads = num_heads
        self.attention = MultiHeadAttention(
            key_dim=embed_dim // num_heads,
            num_heads=num_heads,
            dropout=0.1
        )
        self.norm1 = LayerNormalization(epsilon=1e-6)

    def call(self, inputs):
        x = self.attention(inputs, inputs)
        x = self.norm1(x + inputs)
        return x

class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadAttentionLayer(embed_dim, num_heads)
        self.ffn = tf.keras.Sequential([
            Dense(ff_dim, activation='relu'),
            Dense(embed_dim),
        ])
        self.norm1 = LayerNormalization(epsilon=1e-6)
        self.norm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, inputs):
        x = self.norm1(inputs)
        x = self.att(x)
        x = self.dropout1(x)
        x = self.norm2(x + inputs)
        x = self.ffn(x)
        x = self.dropout2(x)
        x = self.norm2(x + x)
        return x

class DBN(tf.keras.Model):
    def __init__(self, num_transformer_blocks, embed_dim, num_heads, ff_dim, num_classes, mlp_units, dropout=0.1):
        super(DBN, self).__init__()
        self.embedding = Embedding(input_dim=num_classes, output_dim=embed_dim)
        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_transformer_blocks)]
        self.pooling = GlobalAveragePooling1D()
        self.mlp = tf.keras.Sequential([
            Dense(unit, activation='relu') for unit in mlp_units
        ])
        self.classifier = Dense(num_classes, activation='softmax')

    def call(self, inputs):
        x = self.embedding(inputs)
        for transformer_block in self.transformer_blocks:
            x = transformer_block(x)
        x = self.pooling(x)
        x = self.mlp(x)
        x = self.classifier(x)
        return x

import tensorflow as tf

class CombinedModel(tf.keras.Model):
    def __init__(self, vae_model, darknet_model, dbn_model):
        super(CombinedModel, self).__init__()
        self.vae_model = vae_model
        self.darknet_model = darknet_model
        self.dbn_model = dbn_model

    def call(self, inputs):
        # Assuming 'inputs' is the input data for your problem

        # Pass inputs through VAE
        vae_output = self.vae_model(inputs)

        # Pass inputs through Darknet-19
        darknet_output = self.darknet_model(inputs)

        # Pass inputs through DBN
        dbn_output = self.dbn_model(inputs)

        # Combine the outputs in a way that makes sense for your problem
        # For example, you could concatenate, sum, or perform some other operation

        # Example: Concatenate outputs
        combined_output = tf.concat([vae_output, darknet_output, dbn_output], axis=-1)

        # You might want to add additional layers or processing here

        # Return the final output
        return combined_output

vae_model= encoder_reconstructed



combined_model = CombinedModel(vae_model, darknet_model, dbn_model)
combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])